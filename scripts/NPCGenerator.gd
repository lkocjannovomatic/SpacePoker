extends Node

# NPCGenerator.gd - Handles NPC generation using LLM

# Signals
signal generation_completed(slot_index: int, npc_data: Dictionary)
signal generation_failed(error_message: String)

# Constants
const PROMPTS_DIR = "prompts/"

# Generation state
var _is_generating: bool = false
var _current_slot_index: int = -1
var _temp_npc_data: Dictionary = {}

func _ready():
	print("NPCGenerator: Ready")

func is_generating() -> bool:
	"""Check if generation is currently in progress."""
	return _is_generating

func _generate_random_traits() -> Dictionary:
	"""
	Generate random personality traits with trimodal distribution.
	Each trait is independently assigned to LOW, MEDIUM, or HIGH range.
	Returns exact float values within the selected range.
	"""
	return {
		"aggression": _generate_trimodal_value(),
		"bluffing": _generate_trimodal_value(),
		"risk_aversion": _generate_trimodal_value()
	}

func _generate_trimodal_value() -> float:
	"""
	Generate a random value using trimodal distribution.
	33.3% chance each for LOW (0.0-0.3), MEDIUM (0.4-0.6), HIGH (0.7-1.0).
	Returns a uniform random value within the selected range.
	"""
	var range_selector = randf()  # 0.0 to 1.0
	
	if range_selector < 0.333:
		# LOW range: 0.0 to 0.3
		return randf_range(0.0, 0.3)
	elif range_selector < 0.666:
		# MEDIUM range: 0.4 to 0.6
		return randf_range(0.4, 0.6)
	else:
		# HIGH range: 0.7 to 1.0
		return randf_range(0.7, 1.0)

func _get_trait_category(value: float) -> String:
	"""
	Map a trait value to its category (LOW, MEDIUM, HIGH).
	Used for generating the LLM prompt.
	"""
	if value <= 0.3:
		return "LOW"
	elif value <= 0.6:
		return "MEDIUM"
	else:
		return "HIGH"

func _get_trait_description(trait_name: String, category: String) -> String:
	"""
	Get human-readable description for a trait category.
	Used in the LLM prompt to provide context.
	"""
	var descriptions = {
		"aggression": {
			"LOW": "Passive player who checks and calls often, rarely raises",
			"MEDIUM": "Balanced approach with selective aggression when appropriate",
			"HIGH": "Very aggressive player who frequently raises and re-raises"
		},
		"bluffing": {
			"LOW": "Honest player who only bets with strong hands",
			"MEDIUM": "Bluffs occasionally in good spots",
			"HIGH": "Frequent bluffer who is deceptive and unpredictable"
		},
		"risk_aversion": {
			"LOW": "Reckless gambler willing to risk chips freely",
			"MEDIUM": "Calculated risk-taker with a balanced approach",
			"HIGH": "Very cautious and protective of chip stack"
		}
	}
	
	return descriptions[trait_name][category]

func generate_npc(slot_index: int, empty_slot_template: Dictionary):
	"""
	Initiate NPC generation for a specific slot.
	Generates random traits first, then asks LLM for name and backstory.
	
	Args:
		slot_index: The slot index to generate for
		empty_slot_template: Empty slot structure to use as base
	"""
	if _is_generating:
		print("NPCGenerator Warning: Generation already in progress")
		return
	
	print("NPCGenerator: Starting NPC generation for slot ", slot_index)
	
	# Store the slot index and initialize temp data
	_current_slot_index = slot_index
	_is_generating = true
	_temp_npc_data = empty_slot_template.duplicate(true)
	
	# STEP 1: Generate random personality traits with trimodal distribution
	var random_traits = _generate_random_traits()
	_temp_npc_data["aggression"] = random_traits["aggression"]
	_temp_npc_data["bluffing"] = random_traits["bluffing"]
	_temp_npc_data["risk_aversion"] = random_traits["risk_aversion"]
	
	print("NPCGenerator: Generated random traits:")
	print("  Aggression: ", random_traits["aggression"], " (", _get_trait_category(random_traits["aggression"]), ")")
	print("  Bluffing: ", random_traits["bluffing"], " (", _get_trait_category(random_traits["bluffing"]), ")")
	print("  Risk Aversion: ", random_traits["risk_aversion"], " (", _get_trait_category(random_traits["risk_aversion"]), ")")
	
	# Connect to LLM signals
	if not LLMClient.response_received.is_connected(_on_llm_response):
		LLMClient.response_received.connect(_on_llm_response)
	
	if not LLMClient.error_occurred.is_connected(_on_llm_error):
		LLMClient.error_occurred.connect(_on_llm_error)
	
	# STEP 2: Create prompt with pre-generated trait values
	var prompt = _create_prompt_with_traits(random_traits)
	
	# STEP 3: Define simplified JSON schema (only name + backstory)
	var json_schema = _get_npc_json_schema()
	
	# STEP 4: Send to LLM with JSON schema
	var success = LLMClient.send_prompt(prompt, json_schema, LLMClient.ModelConfig.NPC_GENERATION)
	
	if not success:
		_on_generation_failed("Failed to initiate NPC generation")

func _get_npc_json_schema() -> String:
	"""
	Returns the simplified JSON schema for NPC generation.
	Only name and backstory are generated by the LLM.
	Personality traits are pre-generated randomly.
	"""
	var schema = {
		"type": "object",
		"properties": {
			"name": {
				"type": "string",
				"description": "Character name (e.g., Commander, Captain, Salvager, Dr. Chen, etc.)"
			},
			"backstory": {
				"type": "string",
				"description": "3-4 sentence backstory that explains their personality traits and poker style"
			}
		},
		"required": ["name", "backstory"]
	}
	
	return JSON.stringify(schema)

func _load_prompt_file(filename: String) -> String:
	"""Load a prompt file from the prompts directory."""
	var prompt_path = "res://" + PROMPTS_DIR + filename
	
	if not FileAccess.file_exists(prompt_path):
		print("NPCGenerator Error: Prompt file not found at ", prompt_path)
		return ""
	
	var file = FileAccess.open(prompt_path, FileAccess.READ)
	if file == null:
		print("NPCGenerator Error: Could not open prompt file: ", prompt_path)
		return ""
	
	var prompt = file.get_as_text()
	file.close()
	
	return prompt

func _create_prompt_with_traits(traits: Dictionary) -> String:
	"""
	Load the prompt template and replace placeholders with actual trait values.
	"""
	var template = _load_prompt_file("npc_generation.txt")
	
	if template == "":
		print("NPCGenerator Error: Failed to load prompt template")
		return ""
	
	# Get categories for each trait
	var agg_category = _get_trait_category(traits["aggression"])
	var bluff_category = _get_trait_category(traits["bluffing"])
	var risk_category = _get_trait_category(traits["risk_aversion"])
	
	# Replace all placeholders
	var prompt = template.replace("{aggression_category}", agg_category)
	prompt = prompt.replace("{aggression_description}", _get_trait_description("aggression", agg_category))
	prompt = prompt.replace("{aggression_value}", str(traits["aggression"]).pad_decimals(2))
	
	prompt = prompt.replace("{bluffing_category}", bluff_category)
	prompt = prompt.replace("{bluffing_description}", _get_trait_description("bluffing", bluff_category))
	prompt = prompt.replace("{bluffing_value}", str(traits["bluffing"]).pad_decimals(2))
	
	prompt = prompt.replace("{risk_aversion_category}", risk_category)
	prompt = prompt.replace("{risk_aversion_description}", _get_trait_description("risk_aversion", risk_category))
	prompt = prompt.replace("{risk_aversion_value}", str(traits["risk_aversion"]).pad_decimals(2))
	
	return prompt

func _on_llm_response(response_text: String):
	"""
	Handle LLM response for NPC generation (simplified JSON format).
	Only extracts name and backstory; traits were pre-generated.
	"""
	print("NPCGenerator: LLM response received, parsing JSON...")
	
	var parsed_data = _parse_npc_json_response(response_text)
	
	if parsed_data.is_empty():
		_on_generation_failed("Failed to parse NPC data from LLM JSON response")
		return
	
	# Store only name and backstory from LLM
	# Traits (aggression, bluffing, risk_aversion) were already set during generate_npc()
	_temp_npc_data["name"] = parsed_data["name"]
	_temp_npc_data["backstory"] = parsed_data["backstory"]
	
	print("NPCGenerator: NPC generation complete!")
	print("  Name: ", _temp_npc_data["name"])
	print("  Backstory: ", _temp_npc_data["backstory"].substr(0, 100), "...")
	print("  Aggression: ", _temp_npc_data["aggression"], " (", _get_trait_category(_temp_npc_data["aggression"]), ")")
	print("  Bluffing: ", _temp_npc_data["bluffing"], " (", _get_trait_category(_temp_npc_data["bluffing"]), ")")
	print("  Risk Aversion: ", _temp_npc_data["risk_aversion"], " (", _get_trait_category(_temp_npc_data["risk_aversion"]), ")")
	
	# Emit success signal with complete data
	var slot_index = _current_slot_index
	var npc_data = _temp_npc_data.duplicate(true)
	
	# Cleanup before emitting signal
	_cleanup_generation()
	
	# Notify completion
	generation_completed.emit(slot_index, npc_data)

func _parse_npc_json_response(response: String) -> Dictionary:
	"""
	Parse simplified JSON response from LLM (only name and backstory).
	Personality traits were already generated randomly before LLM call.
	"""
	# Clean the response by extracting JSON between Phi-3 special tokens
	var cleaned_response = _extract_json_from_response(response)
	
	var json = JSON.new()
	var parse_result = json.parse(cleaned_response)
	
	if parse_result != OK:
		print("NPCGenerator Error: JSON parse error at line ", json.get_error_line(), ": ", json.get_error_message())
		print("Raw response: ", response)
		print("Cleaned response: ", cleaned_response)
		return {}
	
	var data = json.data
	
	# Validate required fields
	if not data is Dictionary:
		print("NPCGenerator Error: LLM response is not a JSON object")
		return {}
	
	if not data.has("name") or data["name"] == "":
		print("NPCGenerator Error: Missing or empty 'name' field")
		return {}
	
	if not data.has("backstory") or data["backstory"] == "":
		print("NPCGenerator Error: Missing or empty 'backstory' field")
		return {}
	
	# Extract and validate only name and backstory
	var result = {}
	result["name"] = str(data["name"]).strip_edges()
	result["backstory"] = str(data["backstory"]).strip_edges()
	
	# Trim backstory to reasonable length
	if result["backstory"].length() > 1000:
		result["backstory"] = result["backstory"].substr(0, 997) + "..."
	
	return result

func _extract_json_from_response(response: String) -> String:
	"""
	Extract JSON content from LLM response by removing Phi-3 prompt echoes.
	JSON result is between the last <|assistant|> and last <|end|> tags.
	"""
	var cleaned = response.strip_edges()
	
	# Normalize line endings
	cleaned = cleaned.replace("\r\n", "\n")
	cleaned = cleaned.replace("\r", "\n")
	
	# Find the last occurrence of <|assistant|> marker
	var assistant_marker = "<|assistant|>"
	var last_assistant_pos = cleaned.rfind(assistant_marker)
	
	if last_assistant_pos != -1:
		# Extract everything after the last <|assistant|>
		cleaned = cleaned.substr(last_assistant_pos + assistant_marker.length())
	
	# Find the last occurrence of <|end|> marker
	var end_marker = "<|end|>"
	var last_end_pos = cleaned.rfind(end_marker)
	
	if last_end_pos != -1:
		# Extract everything before the last <|end|>
		cleaned = cleaned.substr(0, last_end_pos)
	
	# Strip any remaining whitespace
	cleaned = cleaned.strip_edges()
	
	# Log for debugging
	if last_assistant_pos != -1 or last_end_pos != -1:
		print("NPCGenerator: Cleaned LLM response (removed prompt echo)")
		print("  Original length: ", response.length(), " chars")
		print("  Cleaned length: ", cleaned.length(), " chars")
	
	return cleaned

func _on_llm_error(error_message: String):
	"""Handle LLM errors during generation."""
	print("NPCGenerator Error: LLM error - ", error_message)
	_on_generation_failed(error_message)

func _on_generation_failed(error_message: String):
	"""Handle generation failure and cleanup."""
	print("NPCGenerator Error: NPC generation failed - ", error_message)
	_cleanup_generation()
	generation_failed.emit(error_message)

func _cleanup_generation():
	"""Clean up generation state and disconnect signals."""
	_is_generating = false
	_temp_npc_data = {}
	_current_slot_index = -1
	
	# Disconnect signals
	if LLMClient.response_received.is_connected(_on_llm_response):
		LLMClient.response_received.disconnect(_on_llm_response)
	if LLMClient.error_occurred.is_connected(_on_llm_error):
		LLMClient.error_occurred.disconnect(_on_llm_error)
